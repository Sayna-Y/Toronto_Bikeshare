{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgO5bvYhMiTm"
      },
      "source": [
        "**Toronto Bike Share Analysis: ChatBot**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_9Wc2DCitwF"
      },
      "source": [
        "The Purpose of this part of th eproject is to build a chatbot that can answer questions about Bike Share Toronto Ridership Data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmrwjVlvlcBz"
      },
      "source": [
        "**Install the basic packages required for the chatbot**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# uninstall faiss\n",
        "!pip uninstall faiss-gpu faiss faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWTpxxVueOEu",
        "outputId": "17b8a7f8-3594-4869-d782-eac997bf3d87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: faiss-gpu 1.7.2\n",
            "Uninstalling faiss-gpu-1.7.2:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/faiss/*\n",
            "    /usr/local/lib/python3.10/dist-packages/faiss_gpu-1.7.2.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/faiss_gpu.libs/libgfortran-040039e1.so.5.0.0\n",
            "    /usr/local/lib/python3.10/dist-packages/faiss_gpu.libs/libgomp-a34b3233.so.1.0.0\n",
            "    /usr/local/lib/python3.10/dist-packages/faiss_gpu.libs/libquadmath-96973f99.so.0.0.0\n",
            "    /usr/local/lib/python3.10/dist-packages/faiss_gpu.libs/libz-745e0a09.so.1.2.7\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled faiss-gpu-1.7.2\n",
            "\u001b[33mWARNING: Skipping faiss as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping faiss-cpu as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain pandas sentence-transformers openai\n",
        "!pip install -U sentence-transformers\n",
        "!pip install langchain-openai\n",
        "\n",
        "!pip install faiss-gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T9Pp9ayyiiL9",
        "outputId": "a1606154-9b45-4c22-abb8-a02afb6334b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.1.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.47.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.5)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.125)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.5.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.1.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.3.5)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.47.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain-openai) (0.1.125)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain-openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain-openai) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain-openai) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain-openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (0.5.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (4.66.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-openai) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3->langchain-openai) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.0.7)\n",
            "Collecting faiss-gpu\n",
            "  Using cached faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Using cached faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "Installing collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "faiss"
                ]
              },
              "id": "29376ca850284c4eb743a19563309a52"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "mNA15yImly03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import openai\n",
        "from getpass import getpass\n",
        "from langchain_openai import OpenAI\n",
        "from langchain import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.schema import Document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3GIYibSnJMZ",
        "outputId": "6f40309b-74d2-4561-d8a3-47d33db2ceb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Data**"
      ],
      "metadata": {
        "id": "5S3jYpoEl2Bf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, I import a copy of trip_data and filter the data to include only entries from the year 2022 to avoid runnign issues with computation power. The filtered dataset is then saved to Google Drive, ensuring easy access for future use."
      ],
      "metadata": {
        "id": "2CM5k9romdbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y39cqGLV6agy",
        "outputId": "34a79e58-abd1-4525-cce3-5a398b489fe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UfmeYzhxMdu"
      },
      "outputs": [],
      "source": [
        "#import a copy of trip_data\n",
        "# csv_file_path = \"/trip_data_copy.csv\"\n",
        "# trip_data_copy.to_csv(csv_file_path, index=False)\n",
        "\n",
        "# Drop columns that I added during EDA\n",
        "# trip_data_copy = trip_data_copy.drop(columns = ['Day of Week', 'Hour of Day'])\n",
        "\n",
        "# # Filter the dataset to include only rows where the 'Year' column is 2022\n",
        "# gpt_data = trip_data_copy[trip_data_copy['Year'] == 2022]\n",
        "csv_file_path = \"/content/drive/MyDrive/M.Eng Project/gpt_data.csv\"\n",
        "# gpt_data.to_csv(csv_file_path, index=False)\n",
        "gpt_data = pd.read_csv(csv_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embedding Generation**\n",
        "In this step, I will performs data preprocessing and embedding generation using the A100 GPU for enhanced performance. Previously, when using a High RAM instance, processing the embeddings for the 2022 dataset took over 6 hours, with only 10% of the data processed. Using the A100 GPU significantly accelerated this process."
      ],
      "metadata": {
        "id": "bqkrMYkWnsEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available and set the device accordingly\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Check for FAISS GPU version\n",
        "if not hasattr(faiss, 'StandardGpuResources'):\n",
        "    print(\"FAISS GPU version is not available. Please install the GPU version of FAISS using: !pip install faiss-gpu\")\n",
        "else:\n",
        "    print(\"FAISS GPU version is available.\")\n",
        "\n",
        "# File paths for saving/loading\n",
        "embedding_file_path = '/content/drive/MyDrive/M.Eng Project/embeddings.npy'\n",
        "faiss_index_file_path = '/content/drive/MyDrive/M.Eng Project/bike_share_index.faiss'\n",
        "processed_data_file_path = '/content/drive/MyDrive/M.Eng Project/gpt_data_processed.csv'\n",
        "\n",
        "# Check if the embeddings file and FAISS index already exist\n",
        "if os.path.exists(embedding_file_path) and os.path.exists(faiss_index_file_path):\n",
        "    print(\"Loading existing embeddings and FAISS index...\")\n",
        "\n",
        "    # Load the embeddings\n",
        "    embeddings = np.load(embedding_file_path)\n",
        "\n",
        "    # Load the FAISS index\n",
        "    index = faiss.read_index(faiss_index_file_path)\n",
        "\n",
        "    # Load the processed DataFrame\n",
        "    data = pd.read_csv(processed_data_file_path)\n",
        "else:\n",
        "    print(\"Embeddings and FAISS index not found. Generating new embeddings...\")\n",
        "\n",
        "    # Load the CSV file into a DataFrame\n",
        "    data = pd.read_csv('/content/drive/MyDrive/M.Eng Project/gpt_data.csv')\n",
        "\n",
        "    # Combine relevant columns into a single string for embedding\n",
        "    data['combined_text'] = data.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
        "\n",
        "    # Load and move the SentenceTransformer model to the GPU\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')  # Choose a model based on your needs\n",
        "    model = model.to(device)  # Move model to GPU if available\n",
        "\n",
        "    # Compute embeddings using GPU\n",
        "    embeddings = model.encode(data['combined_text'].tolist(),\n",
        "                              batch_size=512,   # Adjust batch size as necessary based on memory capacity\n",
        "                              convert_to_numpy=True,\n",
        "                              show_progress_bar=True,\n",
        "                              device=device)  # Ensure embeddings are computed on the specified device\n",
        "\n",
        "    # Convert embeddings to float32 if they are not, as FAISS requires this\n",
        "    embeddings = np.array(embeddings).astype('float32')\n",
        "\n",
        "    # Save embeddings to file\n",
        "    np.save(embedding_file_path, embeddings)\n",
        "    print(f\"Embeddings saved to {embedding_file_path}\")\n",
        "\n",
        "    # Get the dimension of the embeddings\n",
        "    dimension = embeddings.shape[1]  # Define the dimension variable correctly\n",
        "\n",
        "    # Store embeddings in FAISS using GPU index if GPU resources are available\n",
        "    if hasattr(faiss, 'StandardGpuResources'):\n",
        "        # Initialize FAISS GPU resources\n",
        "        res = faiss.StandardGpuResources()  # Use a single GPU\n",
        "        index_flat = faiss.IndexFlatL2(dimension)  # Create a CPU index with the correct dimension\n",
        "        index = faiss.index_cpu_to_gpu(res, 0, index_flat)  # Transfer index to GPU\n",
        "    else:\n",
        "        print(\"Warning: Using FAISS on CPU. Performance may be degraded.\")\n",
        "        index = faiss.IndexFlatL2(dimension)  # Create a CPU index with the correct dimension\n",
        "\n",
        "    # Add embeddings to the FAISS index\n",
        "    index.add(embeddings)  # Add embeddings to index\n",
        "\n",
        "    # Save the FAISS index and data for later use\n",
        "    faiss.write_index(faiss.index_gpu_to_cpu(index), faiss_index_file_path)\n",
        "    print(f\"FAISS index saved to {faiss_index_file_path}\")\n",
        "\n",
        "    # Save the processed DataFrame to a CSV file\n",
        "    data.to_csv(processed_data_file_path, index=False)\n",
        "    print(f\"Processed data saved to {processed_data_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuxZuW7XimJj",
        "outputId": "f509c15b-d979-4a09-a0fc-030c11e15af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "FAISS GPU version is available.\n",
            "Loading existing embeddings and FAISS index...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a model to generate dense vector embeddings for text data, capturing semantic meaning in a compact form\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKIdl7kMhKLL",
        "outputId": "354db31b-fab7-4509-a100-75708fe9cf76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_overall_trends(data):\n",
        "    # Calculate some general statistics\n",
        "    total_records = len(data)\n",
        "\n",
        "    # Calculate the number of unique bikes used (you can choose another metric)\n",
        "    unique_bikes = data['Bike Id'].nunique()\n",
        "\n",
        "    # Example: Monthly trends (if there's a 'Year' column)\n",
        "    # Convert the 'End Time' to datetime if needed\n",
        "    data['End Time'] = pd.to_datetime(data['End Time'])\n",
        "    monthly_trends = data.resample('M', on='End Time').size()\n",
        "    daily_trends = data.resample('D', on='End Time').size()\n",
        "\n",
        "    # Calculate average trip duration\n",
        "    average_trip_duration = data['Trip Duration'].mean()\n",
        "\n",
        "    # Calculate the most popular start and end stations\n",
        "    popular_start_station = data['Start Station Name'].value_counts().idxmax()\n",
        "    popular_end_station = data['End Station Name'].value_counts().idxmax()\n",
        "\n",
        "    return {\n",
        "        \"total_records\": total_records,\n",
        "        \"unique_bikes\": unique_bikes,\n",
        "        \"monthly_trends\": monthly_trends,\n",
        "        \"daily_trends\": daily_trends,\n",
        "        \"average_trip_duration\": average_trip_duration,\n",
        "        \"popular_start_station\": popular_start_station,\n",
        "        \"popular_end_station\": popular_end_station\n",
        "    }"
      ],
      "metadata": {
        "id": "aa6BCcJ39mR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_faiss_index(query, index, model, data, k=10):\n",
        "    # Generate embedding for the query\n",
        "    query_embedding = model.encode([query])\n",
        "\n",
        "    # Search in the index\n",
        "    distances, indices = index.search(query_embedding, k)  # k: number of results to retrieve\n",
        "\n",
        "    # Retrieve and return relevant data\n",
        "    return data.iloc[indices[0]], distances[0]"
      ],
      "metadata": {
        "id": "IkV1w9AnLtxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the API Key\n",
        "openai.api_key = getpass('Enter your API key: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCsFys3HS4iP",
        "outputId": "9f51c1fe-a7dc-40c5-8572-ccbbbb674079"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Langchain: Create a template for the chatbot to generate responses based on the search results or analysis\n",
        "prompt_template = \"\"\"\n",
        "You are an intelligent chatbot that has knowledge of the toronto bikeshare network and has all the relevant datasets for it. Your role is to provide short yet informed answers regarding insight from the datasets. Carefully Answer the question based on the following information:\n",
        "\n",
        "Information: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "# Initialize the OpenAI LLM\n",
        "llm = OpenAI(temperature=0.5, api_key=openai.api_key)\n",
        "\n",
        "# Load the LangChain for generating responses\n",
        "qa_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=prompt)\n",
        "\n",
        "# Decide which analysis to use based on the query\n",
        "def chatbot_response(question):\n",
        "    if 'trend' in question or 'overview' in question or 'summary' in question or 'overall' in question or 'total' in question:\n",
        "        # Holistic Analysis\n",
        "        trends = analyze_overall_trends(data)\n",
        "        context = f\"Total Records: {trends['total_records']}, Monthly Trends: {trends['monthly_trends']}\"\n",
        "    else:\n",
        "        # Specific Retrieval\n",
        "        relevant_data, distances = search_faiss_index(question, index, model, data)\n",
        "        context = '\\n'.join(relevant_data['combined_text'].tolist())\n",
        "\n",
        "    # Generate response using LangChain\n",
        "    response = qa_chain.run({\n",
        "        \"context\": context,\n",
        "        \"question\": question\n",
        "    })\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "Pku-u0pfLvOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gpt4o: Function to query gpt-4o-2024-08-06\n",
        "def query_gpt4o(prompt):\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-4o-2024-08-06\",\n",
        "        temperature=0.5,\n",
        "        max_tokens=1000,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert data scientist. You have knowledge of bikeshare toronto system. You should always take a moment to think and carefully ensure accuracy before answering any questions.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "ozR0tNVfmktY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate chatbot response\n",
        "keywords = ['trend', 'overview', 'summary', 'overall', 'total', 'dataset', 'all', '', ' ']\n",
        "def chatbot_response(question):\n",
        "    if any(keyword in question for keyword in keywords):\n",
        "        # Holistic Analysis\n",
        "        trends = analyze_overall_trends(data)\n",
        "        context = (\n",
        "            f\"Total Records: {trends['total_records']}\\n\"\n",
        "            f\"Unique Bikes: {trends['unique_bikes']}\\n\"\n",
        "            f\"Monthly Trends: {trends['monthly_trends']}\\n\"\n",
        "            f\"Daily Trends: {trends['daily_trends']}\\n\"\n",
        "            f\"Average Trip Duration: {trends['average_trip_duration']} minutes\\n\"\n",
        "            f\"Most Popular Start Station: {trends['popular_start_station']}\\n\"\n",
        "            f\"Most Popular End Station: {trends['popular_end_station']}\"\n",
        "        )\n",
        "    else:\n",
        "        # Specific Retrieval\n",
        "        relevant_data, distances = search_faiss_index(question, index, model, data)\n",
        "        context = '\\n'.join(relevant_data['combined_text'].tolist())\n",
        "\n",
        "    ''' Langchain Approach\n",
        "    # Convert context to Document object\n",
        "    input_documents = [Document(page_content=context)]\n",
        "\n",
        "    # Create the input for qa_chain\n",
        "    input_data = {\n",
        "        \"input_documents\": input_documents,  # Pass the list of Document objects\n",
        "        \"question\": question\n",
        "    }\n",
        "\n",
        "    # Generate response using LangChain and extract only 'output_text'\n",
        "    response = qa_chain(input_data)\n",
        "    return response['output_text']  # Return only the output_text\n",
        "    '''\n",
        "\n",
        "    # ''' GPT-4o Approach\n",
        "    # Prepare the prompt for GPT-4o\n",
        "    prompt_gpt = f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "\n",
        "    # Query GPT-4o for the response\n",
        "    response = query_gpt4o(prompt_gpt)\n",
        "    return response\n",
        "    # '''\n",
        "\n",
        "\n",
        "# Setting up the command-line chatbot\n",
        "print(\"Welcome to the Bike Share Chatbot! Type 'exit()' to end the chat.\")\n",
        "\n",
        "while True:\n",
        "    # Get user input\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Check if user wants to exit\n",
        "    if user_input.lower() == 'exit()':\n",
        "        print(\"Bot: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Generate response\n",
        "    try:\n",
        "        bot_response = chatbot_response(user_input)\n",
        "        print(f\"Bot: {bot_response}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Bot: Sorry, something went wrong. Error: {e}\")"
      ],
      "metadata": {
        "id": "mMg2Q-evMlJd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb2f6903-72ce-47ba-b4a5-461cb88b4f5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the Bike Share Chatbot! Type 'exit()' to end the chat.\n",
            "Bot: The Toronto Bike Share, also known as Bike Share Toronto, is a public bicycle sharing system in Toronto, Canada. It provides residents and visitors with an accessible and sustainable mode of transportation by offering a network of bicycles and docking stations throughout the city. Users can rent bikes for short trips, typically using a membership or pay-per-use system. The service aims to promote cycling as an efficient, healthy, and environmentally friendly way to travel around Toronto, reducing traffic congestion and encouraging active transportation.\n",
            "Bot: To determine the columns in the dataset, we can infer some potential columns based on the context and typical data collected by bikeshare systems. However, without direct access to the dataset, I can only provide an educated guess. Here are some likely columns:\n",
            "\n",
            "1. **Trip ID**: A unique identifier for each trip.\n",
            "2. **Start Time**: The date and time when a trip begins.\n",
            "3. **End Time**: The date and time when a trip ends.\n",
            "4. **Bike ID**: A unique identifier for each bike.\n",
            "5. **Start Station ID**: A unique identifier for the start station.\n",
            "6. **Start Station Name**: The name of the start station.\n",
            "7. **End Station ID**: A unique identifier for the end station.\n",
            "8. **End Station Name**: The name of the end station.\n",
            "9. **User Type**: The type of user, such as subscriber or customer.\n",
            "10. **Trip Duration**: The duration of the trip in minutes.\n",
            "11. **Month**: The month of the trip, which can be derived from the start or end time.\n",
            "12. **Day**: The day of the trip, which can be derived from the start or end time.\n",
            "\n",
            "These columns are common in bikeshare datasets and align with the information provided in the context. However, the actual dataset may have additional or different columns.\n",
            "Bot: The most popular start station is York St / Queens Quay W.\n",
            "Bot: The average trip duration is approximately 13.35 minutes.\n",
            "Bot: The busiest month of the year for bike users, based on the provided monthly trends data, is August 2022, with 608,963 trips recorded.\n",
            "Bot: The least popular month, based on the number of trips recorded, is January 2023, with only 26 trips.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "Untitled Notebook 2024-02-15 19:16:32",
      "widgets": {}
    },
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}